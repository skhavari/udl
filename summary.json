[
    "This chapter introduces the three main areas of machine learning—supervised, unsupervised, and reinforcement learning—along with a brief primer on AI ethics and the overall structure of the book.",
    "This chapter formally defines the supervised learning framework, where a model's parameters are trained to minimize a loss function that quantifies the mismatch between its predictions and true outputs, using linear regression as a concrete example.",
    "This chapter introduces shallow neural networks, which describe piecewise linear functions capable of approximating arbitrarily complex relationships between multi-dimensional inputs and outputs using a single hidden layer.",
    "This chapter describes deep neural networks, which stack multiple hidden layers to create more complex piecewise linear functions than shallow networks for a given number of parameters.",
    "This chapter explains how to construct loss functions by viewing models as computing probability distributions over outputs and then finding parameters that maximize the likelihood of the training data.",
    "This chapter covers iterative optimization algorithms like gradient descent, stochastic gradient descent (SGD), and Adam, which adjust a model's parameters based on loss function gradients to find values that minimize prediction errors.",
    "This chapter explains how to efficiently compute the gradients needed for training via the backpropagation algorithm and how to properly initialize network parameters to ensure stable learning.",
    "This chapter discusses measuring model performance on test data by analyzing the sources of error—noise, bias, and variance—and introduces the double descent phenomenon and methods for choosing hyperparameters.",
    "This chapter describes regularization techniques, including explicit methods like adding penalty terms to the loss function and implicit methods arising from the training algorithm, which are used to improve a model's generalization to new data.",
    "This chapter introduces convolutional networks, which are specialized for processing grid-like data like images by using shared-parameter convolutional layers that are equivariant to translation.",
    "This chapter introduces residual networks, which use residual connections and batch normalization to enable the stable training of much deeper neural network architectures.",
    "This chapter introduces transformers, a network architecture based on self-attention that processes variable-length sequences by allowing each input element to interact with every other element.",
    "This chapter describes graph neural networks, which are architectures designed to process graph-structured data by updating node embeddings based on aggregated information from their neighbors.",
    "This chapter provides a taxonomy of unsupervised learning models, focusing on generative models that learn to synthesize new data, and discusses the desirable properties and performance metrics for these models.",
    "This chapter describes generative adversarial networks (GANs), which use a generator network to create new samples and a discriminator network to distinguish them from real data, resulting in a competitive training process that produces realistic samples.",
    "This chapter introduces normalizing flows, a type of probabilistic generative model that learns a complex data distribution by applying a sequence of invertible transformations to a simple base density.",
    "This chapter explains variational autoencoders, which are probabilistic generative models that learn a mapping from a latent space to the data space by maximizing a lower bound on the data likelihood.",
    "This chapter introduces diffusion models, which learn to reverse a fixed process of gradually adding noise to data, enabling the generation of high-quality samples by starting with pure noise and progressively denoising it.",
    "This chapter introduces reinforcement learning, a sequential decision-making framework where agents learn actions to maximize rewards, and discusses how deep networks are used in methods like Deep Q-Learning and policy gradients.",
    "This chapter explores the open questions of why deep learning models are surprisingly easy to train and why they generalize well to new data, despite their complexity and overparameterization.",
    "This chapter discusses the ethical implications of AI, including value alignment, algorithmic bias, potential for misuse, and the broader social and professional responsibilities of practitioners in the field."
]
